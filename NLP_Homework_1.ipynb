{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Homework 1",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "txpYfZQKGn0x",
        "outputId": "903bd2b1-9af5-48d8-d3f4-b36beca01496",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efKTmUZuKLyE",
        "outputId": "3f5117c0-55db-4f01-b5af-9f9f959c870d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data = open('/content/Trump2020Speeches.txt')\n",
        "rawtext= data.read()\n",
        "tokens = nltk.word_tokenize(rawtext) \n",
        "len(rawtext)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "176071"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHmy1ezHLaua",
        "outputId": "e8d58242-43b8-4973-9547-cdecbd534b39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "text = nltk.Text(tokens)\n",
        "text.concordance('china')\n",
        "\n",
        "#When we are done, we close the file.\n",
        "data.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Displaying 25 of 31 matches:\n",
            "y have read a couple of things about China . I spoke to President Xi . Terrific\n",
            "rrific president , a great leader of China , spoke to him this morning at lengt\n",
            "r treasury and companies are leaving China because they want to avoid paying th\n",
            "that you are paying , in the case of China , they devalue their currency , that\n",
            "paying anything at all , the case of China , and we are taking billions and bil\n",
            "we have never taken in 10 cents from China , we would lose $ 500 billion a year\n",
            "would lose $ 500 billion a year with China . We rebuilt China . They 've done a\n",
            "llion a year with China . We rebuilt China . They 've done a great job , but th\n",
            "ve taken historic action to confront china 's chronic trading abuses . It shoul\n",
            "e gets further and further away from China as opposed to calling it the Chinese\n",
            "avily infected , but all people from China in late January , which is months ea\n",
            " right for South Korea , 250,000 and China 's entry into the World Trade Organi\n",
            "in the history of our country . When China joined the World Trade Organization \n",
            "y decisive action to ban travel from China and protect Americans from the virus\n",
            " our lifetime . Joe Biden sided with China over America that 's closing the bor\n",
            "icy decision . Biden is a puppet for China . Son walked out with one point , 5 \n",
            "r farmers . We have plenty left over china 's not exactly happy with being bill\n",
            "ed . Where it originated , which was China , but we 're going to go up up , up \n",
            "at nobody believed possible and then China sent us the plague . Thank you very \n",
            "'s running against , is very weak on China and lots of others things . [ Audien\n",
            "ly and these people -- they get in , China will own this country . And they do \n",
            "d there 's a plague coming over from China . Here it comes . [ Laughter ] What \n",
            "ll it the `` Chinese flu , '' the `` China flu . '' Right ? They call it the ``\n",
            "flu . '' Right ? They call it the `` China , '' as opposed to `` Chi- '' -- the\n",
            "' as opposed to `` Chi- '' -- the `` China . '' I 've never seen anything like \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUTWa8mFMPcF",
        "outputId": "985dfb69-e008-4013-9b3f-347b8bbaf320",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "trumptokens = nltk.word_tokenize(rawtext)\n",
        "# Program to generate a random number between 0 and 1363\n",
        "\n",
        "# importing the random module\n",
        "import random\n",
        "porter = nltk.PorterStemmer()\n",
        "lancaster = nltk.LancasterStemmer()\n",
        "rand = random.randint(0,1363)\n",
        "print(rand)\n",
        "print(\"Raw Text: \" +trumptokens[rand])\n",
        "lancaster = nltk.LancasterStemmer()\n",
        "trumpPstem = [porter.stem(t) for t in trumptokens]\n",
        "print(\"Porter Stem: \" +trumpPstem[rand])\n",
        "trumpLstem = [lancaster.stem(t) for t in trumptokens]\n",
        "print(\"Lancaster Stem: \" +trumpLstem[rand])\n",
        "\n",
        "wnl = nltk.WordNetLemmatizer()\n",
        "trumpLemma = [wnl.lemmatize(t) for t in trumptokens]\n",
        "print(\"Lemmatizer: \" + trumpLemma[rand])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "656\n",
            "Raw Text: that\n",
            "Porter Stem: that\n",
            "Lancaster Stem: that\n",
            "Lemmatizer: that\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw8DCwzRNEzB"
      },
      "source": [
        "def alpha_filter(w):\n",
        "  # pattern to match word of non-alphabetical characters\n",
        "  pattern = re.compile('^[^a-z]+$')\n",
        "  if (pattern.match(w)):\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PaTSrhVNMJh"
      },
      "source": [
        "# tokenize by the regular word tokenizer\n",
        "filetokens = nltk.word_tokenize(rawtext)\n",
        "filetokens = trumpLemma\n",
        "# choose to treat upper and lower case the same\n",
        "#    by putting all tokens in lower case\n",
        "filewords = [w.lower() for w in filetokens]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTtk9_v_2JXX"
      },
      "source": [
        "# apply the function to trumpwords\n",
        "alphatrumpwords = [w for w in filewords if not alpha_filter(w)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2dORIRmNWtS",
        "outputId": "0987a972-8713-4055-9f57-f4bab86084cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltkstopwords = nltk.corpus.stopwords.words('english')\n",
        "import re\n",
        "morestopwords = ['could','would','might','must','need','sha','wo','y',\"'s\",\"'d\",\"'ll\",\"'t\",\"'m\",\"'re\",\"'ve\", \"n't\",'wa','gon','na''audience','boos','boo','chant','audience','applause']\n",
        "stopwords = nltkstopwords + morestopwords\n",
        "stoppedtrumpwords = [w for w in alphatrumpwords if not w in stopwords]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6pX9rTs-8m6",
        "outputId": "b4a42c4a-bab2-4051-9919-c05f1ae15907",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "# use this list for a better frequency distribution\n",
        "from nltk import FreqDist\n",
        "trumpdist = FreqDist(stoppedtrumpwords)\n",
        "trumpitems = trumpdist.most_common(50)\n",
        "for item in trumpitems:\n",
        "  print(item[0],item[1],round(item[1]/len(rawtext),4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "people 199 0.0011\n",
            "great 179 0.001\n",
            "going 177 0.001\n",
            "know 174 0.001\n",
            "said 145 0.0008\n",
            "one 140 0.0008\n",
            "want 138 0.0008\n",
            "thank 129 0.0007\n",
            "like 119 0.0007\n",
            "country 118 0.0007\n",
            "right 116 0.0007\n",
            "get 108 0.0006\n",
            "say 94 0.0005\n",
            "year 93 0.0005\n",
            "job 85 0.0005\n",
            "think 83 0.0005\n",
            "american 81 0.0005\n",
            "lot 76 0.0004\n",
            "never 75 0.0004\n",
            "president 74 0.0004\n",
            "america 74 0.0004\n",
            "time 71 0.0004\n",
            "got 70 0.0004\n",
            "ever 67 0.0004\n",
            "good 63 0.0004\n",
            "way 63 0.0004\n",
            "go 61 0.0003\n",
            "u 61 0.0003\n",
            "many 60 0.0003\n",
            "thing 59 0.0003\n",
            "ha 56 0.0003\n",
            "see 54 0.0003\n",
            "much 54 0.0003\n",
            "let 52 0.0003\n",
            "back 51 0.0003\n",
            "two 51 0.0003\n",
            "even 51 0.0003\n",
            "history 49 0.0003\n",
            "done 49 0.0003\n",
            "make 49 0.0003\n",
            "love 47 0.0003\n",
            "million 47 0.0003\n",
            "first 46 0.0003\n",
            "left 46 0.0003\n",
            "big 45 0.0003\n",
            "come 45 0.0003\n",
            "well 45 0.0003\n",
            "day 44 0.0002\n",
            "every 44 0.0002\n",
            "nobody 43 0.0002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zc5pxzqWa4Qb",
        "outputId": "38bcf419-f7d6-4bb4-98a6-df6536e00145",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# use this list for a better frequency distribution\n",
        "from nltk import FreqDist\n",
        "trumpdist = FreqDist(stoppedtrumpwords)\n",
        "trumpitems = trumpdist.most_common(50)\n",
        "import pandas as pd\n",
        "print(trumpitems)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('people', 199), ('great', 179), ('going', 177), ('know', 174), ('said', 145), ('one', 140), ('want', 138), ('thank', 129), ('like', 119), ('country', 118), ('right', 116), ('get', 108), ('say', 94), ('year', 93), ('job', 85), ('think', 83), ('american', 81), ('lot', 76), ('never', 75), ('president', 74), ('america', 74), ('time', 71), ('got', 70), ('ever', 67), ('good', 63), ('way', 63), ('go', 61), ('u', 61), ('many', 60), ('thing', 59), ('ha', 56), ('see', 54), ('much', 54), ('let', 52), ('back', 51), ('two', 51), ('even', 51), ('history', 49), ('done', 49), ('make', 49), ('love', 47), ('million', 47), ('first', 46), ('left', 46), ('big', 45), ('come', 45), ('well', 45), ('day', 44), ('every', 44), ('nobody', 43)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09SkHPZTNayR"
      },
      "source": [
        "# setup to process bigrams\n",
        "from nltk.collocations import *\n",
        "bigram_measures = nltk.collocations.BigramAssocMeasures()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTni9ck9Nd52"
      },
      "source": [
        "finder = BigramCollocationFinder.from_words(filewords)\n",
        "# choose to use both the non-alpha word filter and a stopwords filter\n",
        "finder.apply_word_filter(alpha_filter)\n",
        "finder.apply_word_filter(lambda w: w in stopwords)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1M9WhbOINgxr",
        "outputId": "596b2f62-64b9-45a6-8d6b-064635f7a984",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "source": [
        "# score by frequency and display the top 50 bigrams\n",
        "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
        "print ()\n",
        "print (\"Bigrams from file with top 50 frequencies\")\n",
        "for item in scored[:50]:\n",
        "        print (item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Bigrams from file with top 50 frequencies\n",
            "(('america', 'great'), 0.0005904703224481413)\n",
            "(('united', 'states'), 0.0005904703224481413)\n",
            "(('joe', 'biden'), 0.0005391250770178681)\n",
            "(('great', 'job'), 0.0005134524543027316)\n",
            "(('make', 'america'), 0.0004621072088724584)\n",
            "(('radical', 'left'), 0.0004364345861573218)\n",
            "(('fake', 'news'), 0.00041076196344218524)\n",
            "(('long', 'time'), 0.00033374409529677554)\n",
            "(('november', '3rd'), 0.00030807147258163895)\n",
            "(('year', 'ago'), 0.00030807147258163895)\n",
            "(('mr.', 'president'), 0.00028239884986650237)\n",
            "(('big', 'deal'), 0.0002567262271513658)\n",
            "(('keep', 'america'), 0.0002567262271513658)\n",
            "(('law', 'enforcement'), 0.0002567262271513658)\n",
            "(('stock', 'market'), 0.0002567262271513658)\n",
            "(('young', 'people'), 0.0002567262271513658)\n",
            "(('american', 'flag'), 0.0002310536044362292)\n",
            "(('black', 'lives'), 0.0002310536044362292)\n",
            "(('lives', 'matter'), 0.0002310536044362292)\n",
            "(('number', 'one'), 0.0002310536044362292)\n",
            "(('little', 'bit'), 0.00020538098172109262)\n",
            "(('open', 'border'), 0.00020538098172109262)\n",
            "(('sleepy', 'joe'), 0.00020538098172109262)\n",
            "(('air', 'force'), 0.00017970835900595606)\n",
            "(('bad', 'people'), 0.00017970835900595606)\n",
            "(('first', 'time'), 0.00017970835900595606)\n",
            "(('free', 'speech'), 0.00017970835900595606)\n",
            "(('half', 'year'), 0.00017970835900595606)\n",
            "(('member', 'call'), 0.00017970835900595606)\n",
            "(('president', 'obama'), 0.00017970835900595606)\n",
            "(('sanctuary', 'city'), 0.00017970835900595606)\n",
            "(('supreme', 'court'), 0.00017970835900595606)\n",
            "(('terrible', 'thing'), 0.00017970835900595606)\n",
            "(('trade', 'deal'), 0.00017970835900595606)\n",
            "(('week', 'ago'), 0.00017970835900595606)\n",
            "(('american', 'people'), 0.00015403573629081948)\n",
            "(('black', 'community'), 0.00015403573629081948)\n",
            "(('border', 'wall'), 0.00015403573629081948)\n",
            "(('brand', 'new'), 0.00015403573629081948)\n",
            "(('four', 'year'), 0.00015403573629081948)\n",
            "(('get', 'rid'), 0.00015403573629081948)\n",
            "(('great', 'people'), 0.00015403573629081948)\n",
            "(('hillary', 'clinton'), 0.00015403573629081948)\n",
            "(('jim', 'inhofe'), 0.00015403573629081948)\n",
            "(('mail-in', 'ballot'), 0.00015403573629081948)\n",
            "(('mark', 'meadows'), 0.00015403573629081948)\n",
            "(('one', 'case'), 0.00015403573629081948)\n",
            "(('people', 'like'), 0.00015403573629081948)\n",
            "(('said', 'general'), 0.00015403573629081948)\n",
            "(('second', 'amendment'), 0.00015403573629081948)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfcqTBaR_65o",
        "outputId": "d3ecc50f-57eb-48b8-ee58-9eecb9b8314b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "# to get good results, must first apply frequency filter\n",
        "finder.apply_freq_filter(5)\n",
        "scored = finder.score_ngrams(bigram_measures.pmi)\n",
        "print (\"Bigrams from file with top 50 by Mutual Information:\")\n",
        "for bscore in scored[:50]:\n",
        "    print (bscore)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bigrams from file with top 50 by Mutual Information:\n",
            "(('matt', 'gaetz'), 12.079484783826818)\n",
            "(('mark', 'meadows'), 12.079484783826814)\n",
            "(('supreme', 'court'), 12.079484783826814)\n",
            "(('stock', 'market'), 11.927481690381766)\n",
            "(('second', 'amendment'), 11.78997816663183)\n",
            "(('november', '3rd'), 11.664447284547972)\n",
            "(('hillary', 'clinton'), 11.567585745295382)\n",
            "(('mail-in', 'ballot'), 11.548970067128035)\n",
            "(('air', 'force'), 11.301877205163262)\n",
            "(('health', 'insurance'), 11.24940978526913)\n",
            "(('jim', 'inhofe'), 11.161946944018789)\n",
            "(('lives', 'matter'), 11.001482271825543)\n",
            "(('west', 'point'), 10.789978166631832)\n",
            "(('united', 'states'), 10.603046739883828)\n",
            "(('fake', 'news'), 10.542050653188246)\n",
            "(('law', 'enforcement'), 10.526943760798037)\n",
            "(('little', 'bit'), 10.442054863211526)\n",
            "(('black', 'lives'), 10.24940978526913)\n",
            "(('health', 'care'), 9.927481690381768)\n",
            "(('sanctuary', 'city'), 9.88683970588442)\n",
            "(('brand', 'new'), 9.779089850489093)\n",
            "(('sleepy', 'joe'), 9.580524801002882)\n",
            "(('trade', 'deal'), 9.311930869827187)\n",
            "(('tax', 'cut'), 9.31081032993327)\n",
            "(('member', 'call'), 9.12012676832416)\n",
            "(('mr.', 'president'), 9.039956419640179)\n",
            "(('vice', 'president'), 9.039956419640179)\n",
            "(('joe', 'biden'), 9.006009087716839)\n",
            "(('radical', 'left'), 8.955329675334884)\n",
            "(('free', 'speech'), 8.940420746089263)\n",
            "(('open', 'border'), 8.867866834084545)\n",
            "(('hundred', 'percent'), 8.605553595494404)\n",
            "(('young', 'woman'), 8.548970067128037)\n",
            "(('black', 'community'), 8.512444191102922)\n",
            "(('american', 'flag'), 8.379045065685725)\n",
            "(('president', 'obama'), 8.262348840976625)\n",
            "(('long', 'time'), 8.215139883184383)\n",
            "(('terrible', 'thing'), 8.174121657964891)\n",
            "(('border', 'wall'), 8.118410295735139)\n",
            "(('week', 'ago'), 8.108397475742054)\n",
            "(('even', 'close'), 8.091557616127393)\n",
            "(('seven', 'year'), 8.03217906904846)\n",
            "(('big', 'deal'), 7.794082564964569)\n",
            "(('get', 'rid'), 7.620053165189518)\n",
            "(('make', 'america'), 7.595171576967283)\n",
            "(('year', 'ago'), 7.59477375674116)\n",
            "(('beautiful', 'border'), 7.5434318835866065)\n",
            "(('half', 'year'), 7.517605896218701)\n",
            "(('black', 'life'), 7.4014128787141775)\n",
            "(('four', 'year'), 7.207750633631912)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJFDbUV_AIPg",
        "outputId": "2e3ed550-905a-41ef-d747-0b19d0a2ed2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
        "finder = TrigramCollocationFinder.from_words(filewords)\n",
        "# choose to use both the non-alpha word filter and a stopwords filter\n",
        "finder.apply_word_filter(alpha_filter)\n",
        "finder.apply_word_filter(lambda w: w in stopwords)\n",
        "# score by frequency and display the top 50 bigrams\n",
        "scored = finder.score_ngrams(trigram_measures.raw_freq)\n",
        "print ()\n",
        "print (\"Trigrams from file with top 20 frequencies\")\n",
        "for item in scored[:20]:\n",
        "        print (item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Trigrams from file with top 20 frequencies\n",
            "(('make', 'america', 'great'), 0.00028239884986650237)\n",
            "(('black', 'lives', 'matter'), 0.0002310536044362292)\n",
            "(('keep', 'america', 'great'), 0.0002310536044362292)\n",
            "(('beautiful', 'border', 'wall'), 0.0001283631135756829)\n",
            "(('air', 'force', 'one'), 0.00010269049086054631)\n",
            "(('criminal', 'justice', 'reform'), 0.00010269049086054631)\n",
            "(('never', 'seen', 'anything'), 0.00010269049086054631)\n",
            "(('seen', 'anything', 'like'), 0.00010269049086054631)\n",
            "(('two', 'great', 'senators'), 0.00010269049086054631)\n",
            "(('good', 'third', 'quarter'), 7.701786814540974e-05)\n",
            "(('great', 'american', 'flag'), 7.701786814540974e-05)\n",
            "(('groundbreaking', 'criminal', 'justice'), 7.701786814540974e-05)\n",
            "(('joe', 'biden', 'ha'), 7.701786814540974e-05)\n",
            "(('law', 'abiding', 'citizen'), 7.701786814540974e-05)\n",
            "(('lives', 'matter', 'movement'), 7.701786814540974e-05)\n",
            "(('number', 'one', 'show'), 7.701786814540974e-05)\n",
            "(('president', 'mike', 'pence'), 7.701786814540974e-05)\n",
            "(('seven', 'year', 'ago'), 7.701786814540974e-05)\n",
            "(('support', 'sanctuary', 'city'), 7.701786814540974e-05)\n",
            "(('supreme', 'court', 'justice'), 7.701786814540974e-05)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfztnptAAeMD",
        "outputId": "d1125a78-251c-49b2-ef6b-1805165ddd62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# to get good results, must first apply frequency filter\n",
        "finder.apply_freq_filter(5)\n",
        "scored = finder.score_ngrams(trigram_measures.pmi)\n",
        "for bscore in scored[:20]:\n",
        "    print (bscore)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(('black', 'lives', 'matter'), 21.250892057094667)\n",
            "(('beautiful', 'border', 'wall'), 17.934860673728156)\n",
            "(('make', 'america', 'great'), 14.650272202167141)\n",
            "(('keep', 'america', 'great'), 14.5831580063086)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Amk3s-XGA0g0",
        "outputId": "5456dc81-3bf1-4d59-b560-eedee14e919e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "from nltk.metrics.association import QuadgramAssocMeasures\n",
        "quadgram_measures = nltk.collocations.QuadgramCollocationFinder\n",
        "finder = QuadgramCollocationFinder.from_words(filewords)\n",
        "# choose to use both the non-alpha word filter and a stopwords filter\n",
        "finder.apply_word_filter(alpha_filter)\n",
        "finder.apply_word_filter(lambda w: w in stopwords)\n",
        "# score by frequency and display the top 50 bigrams\n",
        "scored = finder.score_ngrams(quadgram_measures.apply_freq_filter)\n",
        "print ()\n",
        "print (\"Quadgrams from file with top 20 frequencies\")\n",
        "for item in scored[:20]:\n",
        "        print (item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-ff5793c89732>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_word_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# score by frequency and display the top 50 bigrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mscored\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_ngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquadgram_measures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_freq_filter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Quadgrams from file with top 20 frequencies\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/collocations.py\u001b[0m in \u001b[0;36mscore_ngrams\u001b[0;34m(self, score_fn)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mlowest\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdetermined\u001b[0m \u001b[0mby\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mscoring\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mprovided\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \"\"\"\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_score_ngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnbest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/collocations.py\u001b[0m in \u001b[0;36m_score_ngrams\u001b[0;34m(self, score_fn)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \"\"\"\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngram_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_ngram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/collocations.py\u001b[0m in \u001b[0;36mscore_ngram\u001b[0;34m(self, score_fn, w1, w2, w3, w4)\u001b[0m\n\u001b[1;32m    335\u001b[0m                         \u001b[0;34m(\u001b[0m\u001b[0mn_iixx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_ixix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_ixxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_xixi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_xxii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_xiix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                         \u001b[0;34m(\u001b[0m\u001b[0mn_ixxx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_xixx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_xxix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_xxxi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m                         n_all)\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: apply_freq_filter() takes 2 positional arguments but 5 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCk_XGeUIOXR",
        "outputId": "f949f268-e664-4d8c-95fe-65c2d5c06dfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data = open('/content/TheDarkKnight.txt')\n",
        "rawtext= data.read()\n",
        "tokens = nltk.word_tokenize(rawtext) \n",
        "len(rawtext)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "182507"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzEp4hL5JXzg",
        "outputId": "b5f4eb15-8a9e-4b7d-8950-7530d0f7b505",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "text = nltk.Text(tokens)\n",
        "text.concordance('china')\n",
        "\n",
        "#When we are done, we close the file.\n",
        "data.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Displaying 2 of 2 matches:\n",
            " . DENT Ceramic 28 caliber . Made in China . If you want to kill a public serva\n",
            "s , CEO of L.S.I . Holdings . LAU In China L.S.I . Holdings stands for dynamic \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8oMeBbUKMOI",
        "outputId": "4c5b48f3-42fc-4704-f0e8-c5d485d3ebe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "battokens = nltk.word_tokenize(rawtext)\n",
        "# Program to generate a random number between 0 and 1363\n",
        "\n",
        "# importing the random module\n",
        "import random\n",
        "porter = nltk.PorterStemmer()\n",
        "lancaster = nltk.LancasterStemmer()\n",
        "rand = random.randint(0,1363)\n",
        "print(rand)\n",
        "print(\"Raw Text: \" +battokens[rand])\n",
        "lancaster = nltk.LancasterStemmer()\n",
        "batPstem = [porter.stem(t) for t in battokens]\n",
        "print(\"Porter Stem: \" +batPstem[rand])\n",
        "batLstem = [lancaster.stem(t) for t in battokens]\n",
        "print(\"Lancaster Stem: \" +batLstem[rand])\n",
        "\n",
        "wnl = nltk.WordNetLemmatizer()\n",
        "batLemma = [wnl.lemmatize(t) for t in battokens]\n",
        "print(\"Lemmatizer: \" + batLemma[rand])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "152\n",
            "Raw Text: KIT\n",
            "Porter Stem: kit\n",
            "Lancaster Stem: kit\n",
            "Lemmatizer: KIT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGQW6hoGKtr3"
      },
      "source": [
        "# tokenize by the regular word tokenizer\n",
        "filetokens = nltk.word_tokenize(rawtext)\n",
        "filetokens = batLemma\n",
        "# choose to treat upper and lower case the same\n",
        "#    by putting all tokens in lower case\n",
        "filewords = [w.lower() for w in filetokens]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1UPuEI3Ks_u"
      },
      "source": [
        "# apply the function to batwords\n",
        "alphabatwords = [w for w in filewords if not alpha_filter(w)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7HK90NvK7GW",
        "outputId": "f57f8f6c-29a7-4627-c531-d50adbe1266c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltkstopwords = nltk.corpus.stopwords.words('english')\n",
        "import re\n",
        "morestopwords = ['could','would','might','must','need','sha','wo','y',\"'s\",\"'d\",\"'ll\",\"'t\",\"'m\",\"'re\",\"'ve\", \"n't\",\"int\",\"ext\",\"gon\",\"na\",\"o.s\",\"cont\",\"continuous\"]\n",
        "stopwords = nltkstopwords + morestopwords\n",
        "stoppedbatwords = [w for w in alphabatwords if not w in stopwords]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZ9RFPLlLFd0",
        "outputId": "7e4ccc01-c8c8-4d2c-97f2-4b980ef17a32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "# use this list for a better frequency distribution\n",
        "from nltk import FreqDist\n",
        "batdist = FreqDist(stoppedbatwords)\n",
        "batitems = batdist.most_common(50)\n",
        "for item in batitems:\n",
        "  print(item[0],item[1],round(item[1]/len(rawtext),4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dent 412 0.0023\n",
            "joker 330 0.0018\n",
            "batman 324 0.0018\n",
            "gordon 317 0.0017\n",
            "wayne 255 0.0014\n",
            "look 183 0.001\n",
            "rachel 173 0.0009\n",
            "night 129 0.0007\n",
            "alfred 110 0.0006\n",
            "fox 110 0.0006\n",
            "gotham 107 0.0006\n",
            "one 102 0.0006\n",
            "get 88 0.0005\n",
            "day 83 0.0005\n",
            "know 79 0.0004\n",
            "back 77 0.0004\n",
            "harvey 72 0.0004\n",
            "lau 72 0.0004\n",
            "like 69 0.0004\n",
            "car 66 0.0004\n",
            "turn 64 0.0004\n",
            "pull 64 0.0004\n",
            "want 62 0.0003\n",
            "building 59 0.0003\n",
            "room 59 0.0003\n",
            "take 58 0.0003\n",
            "going 55 0.0003\n",
            "man 54 0.0003\n",
            "bank 53 0.0003\n",
            "street 51 0.0003\n",
            "men 51 0.0003\n",
            "maroni 51 0.0003\n",
            "wa 50 0.0003\n",
            "ramirez 48 0.0003\n",
            "chechen 48 0.0003\n",
            "ferry 48 0.0003\n",
            "go 47 0.0003\n",
            "got 47 0.0003\n",
            "head 47 0.0003\n",
            "people 46 0.0003\n",
            "let 44 0.0002\n",
            "door 44 0.0002\n",
            "cop 44 0.0002\n",
            "hand 43 0.0002\n",
            "gun 42 0.0002\n",
            "phone 42 0.0002\n",
            "penthouse 41 0.0002\n",
            "around 40 0.0002\n",
            "two 39 0.0002\n",
            "reese 39 0.0002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4aVC3DlLcbS"
      },
      "source": [
        "finder = BigramCollocationFinder.from_words(filewords)\n",
        "# choose to use both the non-alpha word filter and a stopwords filter\n",
        "finder.apply_word_filter(alpha_filter)\n",
        "finder.apply_word_filter(lambda w: w in stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcruS84ZLf0N",
        "outputId": "7764b85e-645b-4cd6-adb4-4472f5000760",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "source": [
        "# score by frequency and display the top 50 bigrams\n",
        "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
        "print ()\n",
        "print (\"Bigrams from file with top 50 frequencies\")\n",
        "for item in scored[:50]:\n",
        "        print (item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Bigrams from file with top 50 frequencies\n",
            "(('gotham', 'central'), 0.0007841702172151502)\n",
            "(('prewitt', 'building'), 0.0007580312099746451)\n",
            "(('harvey', 'dent'), 0.0006534751810126252)\n",
            "(('armored', 'car'), 0.0006011971665316151)\n",
            "(('wayne', 'penthouse'), 0.0004966411375695951)\n",
            "(('dent', 'look'), 0.0004443631230885851)\n",
            "(('hong', 'kong'), 0.00041822411584808007)\n",
            "(('bank', 'manager'), 0.0003920851086075751)\n",
            "(('gordon', 'look'), 0.0003920851086075751)\n",
            "(('passenger', 'lounge'), 0.00033980709412656507)\n",
            "(('police', 'van'), 0.00033980709412656507)\n",
            "(('night', 'batman'), 0.00031366808688606005)\n",
            "(('prisoner', 'ferry'), 0.0002875290796455551)\n",
            "(('shotgun', 'swat'), 0.0002875290796455551)\n",
            "(('2nd', 'street'), 0.00026139007240505007)\n",
            "(('cell', 'phone'), 0.00026139007240505007)\n",
            "(('level', 'street'), 0.00026139007240505007)\n",
            "(('look', 'around'), 0.00026139007240505007)\n",
            "(('lower', 'level'), 0.00026139007240505007)\n",
            "(('night', 'gordon'), 0.00026139007240505007)\n",
            "(('fox', 'look'), 0.00023525106516454505)\n",
            "(('gotham', 'streets'), 0.00023525106516454505)\n",
            "(('joker', 'look'), 0.00023525106516454505)\n",
            "(('rooftop', 'overlooking'), 0.00023525106516454505)\n",
            "(('day', 'wayne'), 0.00020911205792404004)\n",
            "(('major', 'crimes'), 0.00020911205792404004)\n",
            "(('mr.', 'wayne'), 0.00020911205792404004)\n",
            "(('night', 'rachel'), 0.00020911205792404004)\n",
            "(('swat', 'team'), 0.00020911205792404004)\n",
            "(('wayne', 'enterprises'), 0.00020911205792404004)\n",
            "(('burnt', 'warehouse'), 0.00018297305068353504)\n",
            "(('commuter', 'ferry'), 0.00018297305068353504)\n",
            "(('crimes', 'unit'), 0.00018297305068353504)\n",
            "(('day', 'fox'), 0.00018297305068353504)\n",
            "(('dent', 'turn'), 0.00018297305068353504)\n",
            "(('insert', 'cut'), 0.00018297305068353504)\n",
            "(('national', 'guard'), 0.00018297305068353504)\n",
            "(('observation', 'room'), 0.00018297305068353504)\n",
            "(('school', 'bus'), 0.00018297305068353504)\n",
            "(('swat', 'leader'), 0.00018297305068353504)\n",
            "(('basement', 'apartment'), 0.00015683404344303003)\n",
            "(('city', 'hall'), 0.00015683404344303003)\n",
            "(('clown', 'mask'), 0.00015683404344303003)\n",
            "(('day', 'gordon'), 0.00015683404344303003)\n",
            "(('good', 'side'), 0.00015683404344303003)\n",
            "(('holding', 'area'), 0.00015683404344303003)\n",
            "(('hospital', 'room'), 0.00015683404344303003)\n",
            "(('interrogation', 'room'), 0.00015683404344303003)\n",
            "(('judge', 'surrillo'), 0.00015683404344303003)\n",
            "(('lower', 'fifth'), 0.00015683404344303003)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmMuNp_bMGV3",
        "outputId": "476ab878-8f9c-4801-e0ff-0c7c41da616e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "# to get good results, must first apply frequency filter\n",
        "finder.apply_freq_filter(5)\n",
        "scored = finder.score_ngrams(bigram_measures.pmi)\n",
        "print (\"Bigrams from file with top 50 by Mutual Information:\")\n",
        "for bscore in scored[:50]:\n",
        "    print (bscore)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bigrams from file with top 50 by Mutual Information:\n",
            "(('major', 'crimes'), 12.223436125820616)\n",
            "(('basement', 'apartment'), 11.831118703041852)\n",
            "(('hong', 'kong'), 11.135973284570275)\n",
            "(('burnt', 'warehouse'), 11.123900452269702)\n",
            "(('insert', 'cut'), 10.97550861237703)\n",
            "(('crimes', 'unit'), 10.943328206627882)\n",
            "(('rooftop', 'overlooking'), 10.720935785291433)\n",
            "(('lower', 'fifth'), 10.579579936045889)\n",
            "(('judge', 'surrillo'), 10.41608120376301)\n",
            "(('holding', 'area'), 10.365455130693041)\n",
            "(('guard', 'commander'), 10.345691875871612)\n",
            "(('passenger', 'lounge'), 10.25853992677653)\n",
            "(('fat', 'thug'), 10.223436125820616)\n",
            "(('school', 'bus'), 10.123900452269702)\n",
            "(('national', 'guard'), 9.938033906958369)\n",
            "(('city', 'hall'), 9.831118703041852)\n",
            "(('lower', 'level'), 9.814045189682913)\n",
            "(('security', 'guard'), 9.693615179291921)\n",
            "(('2nd', 'street'), 9.551010783849119)\n",
            "(('bank', 'manager'), 9.495515671257417)\n",
            "(('commuter', 'ferry'), 9.445828547157065)\n",
            "(('observation', 'room'), 9.340793076458775)\n",
            "(('living', 'room'), 9.340793076458773)\n",
            "(('overlooking', 'prewitt'), 9.316545530212094)\n",
            "(('prewitt', 'building'), 9.291883475977828)\n",
            "(('52nd', 'street'), 9.287976378015326)\n",
            "(('cell', 'phone'), 9.153046797929218)\n",
            "(('swat', 'leader'), 9.12089796410818)\n",
            "(('interrogation', 'room'), 9.118400655122324)\n",
            "(('armored', 'car'), 9.05874777274445)\n",
            "(('prison', 'ferry'), 9.0535111243783)\n",
            "(('police', 'van'), 8.987237904959136)\n",
            "(('prisoner', 'ferry'), 8.84997773029317)\n",
            "(('level', 'street'), 8.785476037486143)\n",
            "(('swat', 'team'), 8.621665337412905)\n",
            "(('clown', 'mask'), 8.60627480271131)\n",
            "(('shotgun', 'swat'), 8.385951537578624)\n",
            "(('gotham', 'central'), 8.344465615669534)\n",
            "(('gotham', 'streets'), 8.066931640140623)\n",
            "(('good', 'side'), 8.04684739409729)\n",
            "(('mr.', 'reese'), 7.6161058120710035)\n",
            "(('hospital', 'room'), 7.402193621122915)\n",
            "(('passenger', 'ferry'), 7.365455130693041)\n",
            "(('wayne', 'enterprises'), 7.229082688961757)\n",
            "(('master', 'wayne'), 6.966048283127963)\n",
            "(('mr.', 'fox'), 6.383182723242387)\n",
            "(('wayne', 'penthouse'), 6.119458197787258)\n",
            "(('look', 'around'), 5.707736287536573)\n",
            "(('dent', 'flips'), 5.688938692082447)\n",
            "(('bruce', 'wayne'), 5.644120188240601)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSKTP6PtMRXq",
        "outputId": "9fb58cfc-2493-43e8-e06d-f26d1395c839",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
        "finder = TrigramCollocationFinder.from_words(filewords)\n",
        "# choose to use both the non-alpha word filter and a stopwords filter\n",
        "finder.apply_word_filter(alpha_filter)\n",
        "finder.apply_word_filter(lambda w: w in stopwords)\n",
        "# score by frequency and display the top 50 bigrams\n",
        "scored = finder.score_ngrams(trigram_measures.raw_freq)\n",
        "print ()\n",
        "print (\"Trigrams from file with top 20 frequencies\")\n",
        "for item in scored[:20]:\n",
        "        print (item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Trigrams from file with top 20 frequencies\n",
            "(('lower', 'level', 'street'), 0.00026139007240505007)\n",
            "(('major', 'crimes', 'unit'), 0.00018297305068353504)\n",
            "(('overlooking', 'prewitt', 'building'), 0.00015683404344303003)\n",
            "(('rooftop', 'overlooking', 'prewitt'), 0.00015683404344303003)\n",
            "(('national', 'guard', 'commander'), 0.00013069503620252504)\n",
            "(('commuter', 'ferry', 'night'), 7.841702172151501e-05)\n",
            "(('eye', 'go', 'wide'), 7.841702172151501e-05)\n",
            "(('hong', 'kong', 'detective'), 7.841702172151501e-05)\n",
            "(('applied', 'sciences', 'division'), 5.227801448101001e-05)\n",
            "(('bank', 'manager', 'fires'), 5.227801448101001e-05)\n",
            "(('bullet', 'fragment', 'array'), 5.227801448101001e-05)\n",
            "(('day', 'rachel', 'come'), 5.227801448101001e-05)\n",
            "(('dogs', 'start', 'barking'), 5.227801448101001e-05)\n",
            "(('evening', 'gordon', 'enters'), 5.227801448101001e-05)\n",
            "(('find', 'harvey', 'dent'), 5.227801448101001e-05)\n",
            "(('garbage', 'truck', 'push'), 5.227801448101001e-05)\n",
            "(('hong', 'kong', 'harbor'), 5.227801448101001e-05)\n",
            "(('joker', 'look', 'back'), 5.227801448101001e-05)\n",
            "(('live', 'long', 'enough'), 5.227801448101001e-05)\n",
            "(('long', 'time', 'ago'), 5.227801448101001e-05)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyOuFrqNMa6H",
        "outputId": "d912e4aa-99a2-4fa3-cbce-378779a0b26e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# to get good results, must first apply frequency filter\n",
        "finder.apply_freq_filter(5)\n",
        "scored = finder.score_ngrams(trigram_measures.pmi)\n",
        "for bscore in scored[:20]:\n",
        "    print (bscore)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(('major', 'crimes', 'unit'), 23.16676433244849)\n",
            "(('national', 'guard', 'commander'), 21.868688283551137)\n",
            "(('rooftop', 'overlooking', 'prewitt'), 20.452518814782373)\n",
            "(('lower', 'level', 'street'), 19.365055973532034)\n",
            "(('overlooking', 'prewitt', 'building'), 18.65733860667087)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2uprnnG5iIC"
      },
      "source": [
        "Biden DNC Speech"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIqFmtHZ5nzz",
        "outputId": "1453bba2-44b8-4833-d14a-ceff0c522e3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data = open('/content/BidenDNCAcceptance.txt')\n",
        "rawtext= data.read()\n",
        "tokens = nltk.word_tokenize(rawtext) \n",
        "len(rawtext)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18360"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFyNeXSL6eKQ",
        "outputId": "9777d6c5-6974-4ce3-ba76-eceead604340",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "text = nltk.Text(tokens)\n",
        "text.concordance('china')\n",
        "\n",
        "#When we are done, we close the file.\n",
        "data.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Displaying 1 of 1 matches:\n",
            " will never again be at the mercy of China or other foreign countries in order \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sy7idGW56m0Y",
        "outputId": "74a399cf-7eee-48cc-da38-9c5ccff8306a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "bidentokens = nltk.word_tokenize(rawtext)\n",
        "# Program to generate a random number between 0 and 1363\n",
        "\n",
        "# importing the random module\n",
        "import random\n",
        "porter = nltk.PorterStemmer()\n",
        "lancaster = nltk.LancasterStemmer()\n",
        "rand = random.randint(0,1363)\n",
        "print(rand)\n",
        "print(\"Raw Text: \" +bidentokens[rand])\n",
        "lancaster = nltk.LancasterStemmer()\n",
        "bidenPstem = [porter.stem(t) for t in bidentokens]\n",
        "print(\"Porter Stem: \" +bidenPstem[rand])\n",
        "bidenLstem = [lancaster.stem(t) for t in bidentokens]\n",
        "print(\"Lancaster Stem: \" +bidenLstem[rand])\n",
        "\n",
        "wnl = nltk.WordNetLemmatizer()\n",
        "bidenLemma = [wnl.lemmatize(t) for t in bidentokens]\n",
        "print(\"Lemmatizer: \" + bidenLemma[rand])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "113\n",
            "Raw Text: And\n",
            "Porter Stem: and\n",
            "Lancaster Stem: and\n",
            "Lemmatizer: And\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQS2utGx68_4"
      },
      "source": [
        "# tokenize by the regular word tokenizer\n",
        "filetokens = nltk.word_tokenize(rawtext)\n",
        "filetokens = bidenLemma\n",
        "# choose to treat upper and lower case the same\n",
        "#    by putting all tokens in lower case\n",
        "filewords = [w.lower() for w in filetokens]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3oK279Q7JGO"
      },
      "source": [
        "# apply the function to bidenwords\n",
        "alphabidenwords = [w for w in filewords if not alpha_filter(w)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2oil59O7S-C",
        "outputId": "f3f7a609-cf9f-4b9c-e0a8-998bd0feb26c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltkstopwords = nltk.corpus.stopwords.words('english')\n",
        "import re\n",
        "morestopwords = ['could','would','might','must','need','sha','wo','y',\"'s\",\"'d\",\"'ll\",\"'t\",\"'m\",\"'re\",\"'ve\", \"n't\",\"int\",\"ext\",\"gon\",\"na\"]\n",
        "stopwords = nltkstopwords + morestopwords\n",
        "stoppedbidenwords = [w for w in alphabidenwords if not w in stopwords]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_xW08pp7vy6",
        "outputId": "3f6c38ea-4df7-4c6a-94f2-31c2fd3ba957",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "# use this list for a better frequency distribution\n",
        "from nltk import FreqDist\n",
        "bidendist = FreqDist(stoppedbidenwords)\n",
        "bidenitems = bidendist.most_common(50)\n",
        "for item in bidenitems:\n",
        "  print(item[0],item[1],round(item[1]/len(rawtext),4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "america 30 0.0016\n",
            "president 29 0.0016\n",
            "one 19 0.001\n",
            "u 18 0.001\n",
            "people 17 0.0009\n",
            "time 14 0.0008\n",
            "make 12 0.0007\n",
            "moment 12 0.0007\n",
            "going 12 0.0007\n",
            "never 12 0.0007\n",
            "nation 12 0.0007\n",
            "get 12 0.0007\n",
            "light 11 0.0006\n",
            "great 11 0.0006\n",
            "united 10 0.0005\n",
            "hope 10 0.0005\n",
            "year 10 0.0005\n",
            "wa 10 0.0005\n",
            "love 9 0.0005\n",
            "know 9 0.0005\n",
            "say 9 0.0005\n",
            "ha 8 0.0004\n",
            "much 8 0.0004\n",
            "together 8 0.0004\n",
            "american 8 0.0004\n",
            "job 8 0.0004\n",
            "country 8 0.0004\n",
            "history 8 0.0004\n",
            "take 8 0.0004\n",
            "million 8 0.0004\n",
            "family 8 0.0004\n",
            "life 8 0.0004\n",
            "world 8 0.0004\n",
            "way 7 0.0004\n",
            "word 7 0.0004\n",
            "hard 7 0.0004\n",
            "back 7 0.0004\n",
            "protect 7 0.0004\n",
            "always 7 0.0004\n",
            "work 6 0.0003\n",
            "promise 6 0.0003\n",
            "believe 6 0.0003\n",
            "said 6 0.0003\n",
            "americans 6 0.0003\n",
            "every 6 0.0003\n",
            "pay 6 0.0003\n",
            "current 5 0.0003\n",
            "fear 5 0.0003\n",
            "new 5 0.0003\n",
            "winning 5 0.0003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAgrjrUO8BRl"
      },
      "source": [
        "finder = BigramCollocationFinder.from_words(filewords)\n",
        "# choose to use both the non-alpha word filter and a stopwords filter\n",
        "finder.apply_word_filter(alpha_filter)\n",
        "finder.apply_word_filter(lambda w: w in stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG42Q4El8OEp",
        "outputId": "e53b6ade-efc8-4520-bda2-d423ac6c16b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "source": [
        "# score by frequency and display the top 50 bigrams\n",
        "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
        "print ()\n",
        "print (\"Bigrams from file with top 50 frequencies\")\n",
        "for item in scored[:50]:\n",
        "        print (item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Bigrams from file with top 50 frequencies\n",
            "(('million', 'people'), 0.0010139416983523447)\n",
            "(('climate', 'change'), 0.0007604562737642585)\n",
            "(('current', 'president'), 0.0007604562737642585)\n",
            "(('president', 'ha'), 0.0007604562737642585)\n",
            "(('social', 'security'), 0.0007604562737642585)\n",
            "(('young', 'people'), 0.0007604562737642585)\n",
            "(('affordable', 'care'), 0.0005069708491761723)\n",
            "(('build', 'back'), 0.0005069708491761723)\n",
            "(('care', 'act'), 0.0005069708491761723)\n",
            "(('daddy', 'changed'), 0.0005069708491761723)\n",
            "(('ever', 'faced'), 0.0005069708491761723)\n",
            "(('every', 'day'), 0.0005069708491761723)\n",
            "(('give', 'people'), 0.0005069708491761723)\n",
            "(('history', 'ha'), 0.0005069708491761723)\n",
            "(('history', 'rhyme'), 0.0005069708491761723)\n",
            "(('may', 'god'), 0.0005069708491761723)\n",
            "(('middle', 'class'), 0.0005069708491761723)\n",
            "(('never', 'get'), 0.0005069708491761723)\n",
            "(('one', 'another'), 0.0005069708491761723)\n",
            "(('people', 'light'), 0.0005069708491761723)\n",
            "(('powerful', 'voice'), 0.0005069708491761723)\n",
            "(('president', 'obama'), 0.0005069708491761723)\n",
            "(('protect', 'america'), 0.0005069708491761723)\n",
            "(('united', 'america'), 0.0005069708491761723)\n",
            "(('united', 'states'), 0.0005069708491761723)\n",
            "(('vice', 'president'), 0.0005069708491761723)\n",
            "(('way', 'forward'), 0.0005069708491761723)\n",
            "(('21st', 'century'), 0.00025348542458808617)\n",
            "(('accelerating', 'threat'), 0.00025348542458808617)\n",
            "(('across', 'europe'), 0.00025348542458808617)\n",
            "(('ago', 'yesterday'), 0.00025348542458808617)\n",
            "(('almost', 'anywhere'), 0.00025348542458808617)\n",
            "(('almost', 'half'), 0.00025348542458808617)\n",
            "(('alone', 'two'), 0.00025348542458808617)\n",
            "(('always', 'believed'), 0.00025348542458808617)\n",
            "(('always', 'got'), 0.00025348542458808617)\n",
            "(('always', 'hear'), 0.00025348542458808617)\n",
            "(('always', 'stand'), 0.00025348542458808617)\n",
            "(('american', 'community'), 0.00025348542458808617)\n",
            "(('american', 'darkness'), 0.00025348542458808617)\n",
            "(('american', 'history'), 0.00025348542458808617)\n",
            "(('american', 'moment'), 0.00025348542458808617)\n",
            "(('american', 'president'), 0.00025348542458808617)\n",
            "(('american', 'soldier'), 0.00025348542458808617)\n",
            "(('american', 'story'), 0.00025348542458808617)\n",
            "(('american', 'worker'), 0.00025348542458808617)\n",
            "(('americans', 'infected'), 0.00025348542458808617)\n",
            "(('among', 'u'), 0.00025348542458808617)\n",
            "(('anti-semitic', 'bile'), 0.00025348542458808617)\n",
            "(('anywhere', 'else'), 0.00025348542458808617)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBlq5i5T8buz",
        "outputId": "e9c83b4f-8abe-4f8e-b085-f6cfc9551df7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# to get good results, must first apply frequency filter\n",
        "finder.apply_freq_filter(5)\n",
        "scored = finder.score_ngrams(bigram_measures.pmi)\n",
        "print (\"Bigrams from file with top 50 by Mutual Information:\")\n",
        "for bscore in scored[:50]:\n",
        "    print (bscore)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bigrams from file with top 50 by Mutual Information:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8nC_kKn8jtg",
        "outputId": "ed47083d-ff18-4480-9ad8-894c39450702",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "source": [
        "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
        "finder = TrigramCollocationFinder.from_words(filewords)\n",
        "# choose to use both the non-alpha word filter and a stopwords filter\n",
        "finder.apply_word_filter(alpha_filter)\n",
        "finder.apply_word_filter(lambda w: w in stopwords)\n",
        "# score by frequency and display the top 50 bigrams\n",
        "scored = finder.score_ngrams(trigram_measures.raw_freq)\n",
        "print ()\n",
        "print (\"Trigrams from file with top 20 frequencies\")\n",
        "for item in scored[:50]:\n",
        "        print (item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Trigrams from file with top 20 frequencies\n",
            "(('affordable', 'care', 'act'), 0.0005069708491761723)\n",
            "(('current', 'president', 'ha'), 0.0005069708491761723)\n",
            "(('give', 'people', 'light'), 0.0005069708491761723)\n",
            "(('almost', 'anywhere', 'else'), 0.00025348542458808617)\n",
            "(('always', 'got', 'back'), 0.00025348542458808617)\n",
            "(('american', 'community', 'bearing'), 0.00025348542458808617)\n",
            "(('american', 'darkness', 'began'), 0.00025348542458808617)\n",
            "(('american', 'history', 'tell'), 0.00025348542458808617)\n",
            "(('american', 'worker', 'ca'), 0.00025348542458808617)\n",
            "(('anti-semitic', 'bile', 'heard'), 0.00025348542458808617)\n",
            "(('bile', 'heard', 'across'), 0.00025348542458808617)\n",
            "(('brave', 'little', 'girl'), 0.00025348542458808617)\n",
            "(('build', 'back', 'better'), 0.00025348542458808617)\n",
            "(('certain', 'inalienable', 'right'), 0.00025348542458808617)\n",
            "(('civil', 'right', 'movement'), 0.00025348542458808617)\n",
            "(('coming', 'year', 'bright'), 0.00025348542458808617)\n",
            "(('decorated', 'iraqi', 'war'), 0.00025348542458808617)\n",
            "(('deep', 'black', 'hole'), 0.00025348542458808617)\n",
            "(('deploy', 'rapid', 'test'), 0.00025348542458808617)\n",
            "(('deserves', 'one', 'great'), 0.00025348542458808617)\n",
            "(('difficult', 'moment', 'america'), 0.00025348542458808617)\n",
            "(('economic', 'crisis', 'since'), 0.00025348542458808617)\n",
            "(('empowered', 'labor', 'union'), 0.00025348542458808617)\n",
            "(('every', 'day', 'believing'), 0.00025348542458808617)\n",
            "(('existential', 'threat', 'posed'), 0.00025348542458808617)\n",
            "(('family', 'back', 'together'), 0.00025348542458808617)\n",
            "(('father', 'taught', 'u'), 0.00025348542458808617)\n",
            "(('four', 'historic', 'crisis'), 0.00025348542458808617)\n",
            "(('franklin', 'roosevelt', 'pledged'), 0.00025348542458808617)\n",
            "(('generation', 'never', 'know'), 0.00025348542458808617)\n",
            "(('generous', 'among', 'u'), 0.00025348542458808617)\n",
            "(('george', 'lloyd', 'wa'), 0.00025348542458808617)\n",
            "(('great', 'first', 'lady'), 0.00025348542458808617)\n",
            "(('great', 'middle', 'class'), 0.00025348542458808617)\n",
            "(('great', 'second', 'lady'), 0.00025348542458808617)\n",
            "(('great', 'vice', 'president'), 0.00025348542458808617)\n",
            "(('ha', 'cloaked', 'america'), 0.00025348542458808617)\n",
            "(('ha', 'delivered', 'u'), 0.00025348542458808617)\n",
            "(('ha', 'thrust', 'one'), 0.00025348542458808617)\n",
            "(('heard', 'across', 'europe'), 0.00025348542458808617)\n",
            "(('history', 'ha', 'delivered'), 0.00025348542458808617)\n",
            "(('history', 'ha', 'thrust'), 0.00025348542458808617)\n",
            "(('history', 'tell', 'u'), 0.00025348542458808617)\n",
            "(('honest', 'unvarnished', 'truth'), 0.00025348542458808617)\n",
            "(('inalienable', 'right', 'among'), 0.00025348542458808617)\n",
            "(('incredibly', 'brave', 'little'), 0.00025348542458808617)\n",
            "(('iraqi', 'war', 'veteran'), 0.00025348542458808617)\n",
            "(('keep', 'telling', 'u'), 0.00025348542458808617)\n",
            "(('kid', 'safely', 'back'), 0.00025348542458808617)\n",
            "(('last', 'four', 'year'), 0.00025348542458808617)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM7EC6RJ8ypy"
      },
      "source": [
        "# to get good results, must first apply frequency filter\n",
        "finder.apply_freq_filter(5)\n",
        "scored = finder.score_ngrams(trigram_measures.pmi)\n",
        "for bscore in scored[:20]:\n",
        "    print (bscore)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}